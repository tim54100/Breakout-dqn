{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from DQN_N import DeepQNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pltin(x, y, x_name, y_name, file_path):\n",
    "    plt.figure()\n",
    "    plt.plot(x, y,'--*b')\n",
    "    plt.xlabel(x_name)\n",
    "    plt.ylabel(y_name)\n",
    "    plt.savefig(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_step(frames_to_skip, env, step, action):\n",
    "    reward = 0\n",
    "    for i in range(frames_to_skip):\n",
    "        state, r, done, info = step(action)\n",
    "        reward += r \n",
    "        if done:\n",
    "            break\n",
    "    return state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, width, height):\n",
    "    #img = cv2.cvtColor(cv2.resize(img, (width, length)), cv2.COLOR_RGB2GRAY)\n",
    "    img = np.array(img, dtype = np.uint8)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.resize(img, (width, height))\n",
    "#     img = np.vstack(img)\n",
    "    img = img.astype(np.float)\n",
    "    img /= 255\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_networks/saved_dqn-559999\n",
      "Successfully loaded: saved_networks/saved_dqn-559999\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MsPacman-v0')# Breakout-ram-v0\n",
    "\n",
    "width = 84\n",
    "height = 84\n",
    "batch_size = 32\n",
    "memory_size = 80000\n",
    "replace_iter = 4000\n",
    "frame_repeat = 4\n",
    "n_a = env.action_space.n\n",
    "push = False\n",
    "        \n",
    "step = 0    \n",
    "e_step = []\n",
    "e_score = []\n",
    "e_loss = []\n",
    "epsilon = []\n",
    "episodes = 100\n",
    "num_game = 0\n",
    "\n",
    "RL = DeepQNetwork(n_a,\n",
    "                  st_shape = [width, height],\n",
    "                  learning_rate= 0.0001,\n",
    "                  reward_decay =0.99,\n",
    "                  epsilon_start= 1,\n",
    "                  epsilon_end = 0.1,\n",
    "                  explore = 500000,\n",
    "                  replace_target_iter= replace_iter,\n",
    "                  memory_size= memory_size,\n",
    "                  batch_size = batch_size,\n",
    "                  training = True,\n",
    "                  output_graph = False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7469d01bc7924bdf9d7243c21acc07e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c2e3eebaf9c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ale.lives'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlives\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/EastGeno/DQN/DQN_N.py\u001b[0m in \u001b[0;36mmake_policy\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# and the probability is defined by the epsilon and actions amount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/EastGeno/DQN/DQN_N.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m# predict the q_value with state and q_eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_pl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# predict the q_value with state and q_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;34m\"\"\"Perform either run or partial_run, depending the presence of `handle`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_feed_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_REGISTERED_EXPANSIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    total_loss = 0\n",
    "    total_score = 0\n",
    "    with tqdm(total=episodes) as t:\n",
    "        for episode in range(episodes):\n",
    "\n",
    "\n",
    "            action_tracker = np.zeros(n_a)\n",
    "            loss = 0\n",
    "            i=0\n",
    "            score = 0\n",
    "            done = False\n",
    "            lives = -1\n",
    "            info = {}\n",
    "#             info['ale.lives'] = 5\n",
    "            info['ale.lives'] = 3\n",
    "            \n",
    "            s = env.reset()\n",
    "#             s = np.array(s,dtype='float')\n",
    "            s = preprocess(s, width, height)\n",
    "            s = np.stack((s, s, s, s), axis=2)\n",
    "#                 print(s.shape)\n",
    "\n",
    "            while not done:\n",
    "                action = RL.make_policy(s)\n",
    "                \n",
    "                if info['ale.lives'] != lives:\n",
    "                    lives = info['ale.lives']\n",
    "#                     action=1\n",
    "                    #s = True\n",
    "                    \n",
    "                action_tracker[action] += 1\n",
    "                n_s, reward, done, info = skip_step(frame_repeat, env, env.step, action)\n",
    "#                 n_s = np.array(n_s,dtype='float')\n",
    "                n_s = preprocess(n_s, width, height)\n",
    "                n_s = np.append(s[:, :, 1:], np.expand_dims(n_s, 2), axis=2)\n",
    "                score += reward\n",
    "                    \n",
    "                if info['ale.lives'] != lives:\n",
    "                    reward = -10\n",
    "                    #s = True\n",
    "                    \n",
    "                reward /= 10\n",
    "                reward = np.max([-1 , np.min([1, reward])])\n",
    "                RL.store(s, action, reward, n_s, not done)\n",
    "\n",
    "                s = n_s\n",
    "                if (len(RL.replay_memory) > RL.memory_size/4 and step%4 == 0):\n",
    "                    los = RL.learn()\n",
    "#                     print(los)\n",
    "#                     los,TD,p_loss = RL.learn()\n",
    "#                     print(\"TD: %f, p_loss: %f\"%(np.reducemean(TD), p_loss))\n",
    "                    loss += los\n",
    "                    i+=1\n",
    "                step += 1\n",
    "#                     if step %1000 == 0:\n",
    "#                          print('total_step: %d, step: %d, current_score: %f' % (step, i*4, score))\n",
    "\n",
    "            loss = loss/i if (i != 0 ) else 0\n",
    "            total_score += score\n",
    "            total_loss += loss\n",
    "#                 print('episode: %d, epsilon: %f, score: %f, loss: %f'%(num_game+episode+1, RL.epsilon, score, loss))\n",
    "#                 print('action'+'action'.join(str(i)+': '+str(action_tracker[i])[:-2]+'  ' for i in range(len(action_tracker))))\n",
    "\n",
    "\n",
    "            t.set_postfix(score=score, loss = loss, step=step)\n",
    "            t.update()\n",
    "#         if RL.epsilon == RL.epsilon_end and RL.epsilon_end != 0.001:\n",
    "#             RL.learning_rate= 0.0001\n",
    "#             RL.epsilon_start /= 10\n",
    "#             RL.explore = 2000000\n",
    "#             RL.epsilon_end /= 10\n",
    "\n",
    "        num_game += episodes\n",
    "\n",
    "    if push:\n",
    "        e_step.append(RL.learn_step_counter/10000)\n",
    "        e_score.append(total_score/episodes)\n",
    "        epsilon.append(RL.epsilon)\n",
    "        e_loss.append(total_loss/episodes)\n",
    "        path = './picture/'+str(RL.learn_step_counter)\n",
    "        file = open('./saved_networks/train_history.pickle', 'wb')\n",
    "        pickle.dump([e_step, e_score, epsilon, e_loss], file)\n",
    "        file.close()\n",
    "#         pltin(e_step, e_score, 'step(10000)' , 'score', path + '_score.png')\n",
    "#         pltin(e_step, e_loss, 'step(10000)' , 'loss', path + '_loss.png')\n",
    "#         plt.close('all')\n",
    "    if total_loss != 0:\n",
    "        push = True\n",
    "#         if not os.path.exists('./picture'):\n",
    "#             os.makedirs('picture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
